{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sideka : Keyword Extraction for Each \"Desa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indonesia has thousands of villages (Bahasa = \"desa\"), the lowest level of government administration. While often portrayed as a beautiful and comfortable place for its citizens to live in, there are also some aspects of poverty and lag of infrastructure development that needs to be seriously considered. Since 2014, efforts have been made by Governments of Indonesia to improve the social welfare and quality of life as mandated by the Constitution. Unavailability of information is one major aspect contributing to this problem, and the Government, through the Village and Regional Empowerment Initiatives (\"Badan Prakarsa Pemberdayaan Desa dan Kawasan\" - BP2DK) launched \"Sistem Informasi Desa dan Kawasan\" (hereafter referred as \"SIDEKA\").  \n",
    "\n",
    "Until May 2018, SIDEKA has been utilized by around 4956 villages all around Indonesia. It provides a platform for the villages to monitor their activities, which afterward, the data can be compiled and used by the local government to form their Village Mid-Term Development Plans (\"Rencana Pembangunan Jangka Menengah Desa\" - RPJMDes), Village Government Activity Plans (\"Rencana Kegiatan Pemerintah Desa\" - RKPDes), and Village Revenue and Expenditure Budget Plan (\"Rencana Anggaran Pendapatan dan Belanja Desa\" - RAPBDes)\n",
    "\n",
    "Going forward, data from SIDEKA's implementations should be able to be utilized by higher-level administrative government such as District, Province, and Central Government so they can make a more tailored policy for each village according to their \"uniqueness\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems to be Tackled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniqueness. We are gathering all informations available from each village's website - specifically from their \"latest news\" section, compiling them, and extract keywords that are unique and can be a defining characteristic for the village.  \n",
    "  \n",
    "Example : Desa X defining keywords = \"Rengginang Ketan\" ; then we can point out to the policy makers higher ups to put more attention to this keyword since it may mean that aforementioned village economy leans heavily toward producing and selling this \"Rengginang Ketan\". Thus, subsequent policies created for this village should be accomodating to this fact (e.g. put more incentives for the villagers to create and sell more \"Rengginang Ketan\", more tax leeway, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use three methods often used to find the most important words across text documents:\n",
    "\n",
    "1. Word count\n",
    "2. Term Frequency - Inverse Document Frequency\n",
    "3. Rapid Automated Keyword Extraction\n",
    "\n",
    "Document sets used here are collection of articles scraped from Desa Pejeng (http://www.pejeng.desa.id/post/). Python is the language used to implement the aforementioned methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import glob\n",
    "import nltk\n",
    "import operator\n",
    "import csv\n",
    "import argparse\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from pprint import pprint\n",
    "\n",
    "# Import Dictionary, TfidfMode from gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'scraped' article body: desa pejeng\n",
    "source_dir = 'pejeng_articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the news articles, sorted by last modification time: articles\n",
    "file_list = sorted(glob.glob(source_dir + '/*.txt'), key=os.path.getmtime)\n",
    "articles = [open(f, 'r').read() for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess articles: lowercasing and tokenizing all words\n",
    "articles_lower_tokenize = [word_tokenize(t.lower())\n",
    "                           for t in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess articles: removing 'indonesian' stopwords: articles_no_stop\n",
    "stopwords_indonesian = stopwords.words('indonesian')\n",
    "articles_no_stop = [[t for t in sublist if t not in stopwords_indonesian]\n",
    "                    for sublist in articles_lower_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess articles: removing punctuation\n",
    "articles_no_empty = [[t for t in sublist if t]\n",
    "                     for sublist in articles_no_stop]\n",
    "articles_no_empty_intermediate_1 = [[t for t in sublist if '``' not in t]\n",
    "                                    for sublist in articles_no_empty]\n",
    "articles_no_empty_intermediate_2 = [[t for t in sublist if '\\'\\'' not in t]\n",
    "                                    for sublist in articles_no_empty_intermediate_1]\n",
    "articles_cleaned = [[t for t in sublist if t not in punctuation]\n",
    "                    for sublist in articles_no_empty_intermediate_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Top 10 Words according to frequency:\n",
      "----------------------------------------\n",
      "[('pejeng', 88), ('dewa', 66), ('desa', 50), ('banjar', 43), ('suamba', 43), ('salah', 40), ('lapangan', 39), ('pura', 37), ('warga', 37), ('anak-anak', 36)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Looking up top 5 most-common words in the corpora\n",
    "# Create a counter object: counter\n",
    "counter = Counter([word for words in articles_cleaned for word in set(words)])\n",
    "print('-----' * 8)\n",
    "print(\"Top 10 Words according to frequency:\")\n",
    "print('-----' * 8)\n",
    "print(counter.most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Using Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim corpus and then apply Tfidf to that corpus\n",
    "# Create a (gensim) dictionary object from the articles_cleaned: dictionary\n",
    "dictionary = Dictionary(articles_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "TF-IDF Object from Corpus\n",
      "----------------------------------------\n",
      "TfidfModel(num_docs=89, num_nnz=11160) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tfidf object from corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "print('-----' * 8)\n",
    "print(\"TF-IDF Object from Corpus\")\n",
    "print('-----' * 8)\n",
    "print(tfidf, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TFIDF Weights of all terms found in corpus\n",
    "#  print as list of tuples, in descending order \n",
    "# Create a container for the list of tuples: tfidf_tuples\n",
    "tfidf_tuples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the cleaned articles\n",
    "# Get the top-5 of tfidf weight\n",
    "for i in range(len(articles_cleaned)):\n",
    "    doc = corpus[i]\n",
    "    tfidf_weights = tfidf[doc]\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "    #sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1])\n",
    "    #for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    for term_id, weight in sorted_tfidf_weights:\n",
    "        tfidf_tuples.append((dictionary.get(term_id), term_id, weight, 'corpus_{}'.format(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Term and Weight for entire corpora\n",
      "----------------------------------------\n",
      "[('elpiji', 2633, 0.85133269822700652, 'corpus_46'),\n",
      " ('kulkul', 834, 0.78491690292548111, 'corpus_48'),\n",
      " ('pengungsi', 2785, 0.72284623796788605, 'corpus_51'),\n",
      " ('ogoh-ogoh', 858, 0.66108899215597017, 'corpus_8'),\n",
      " ('topeng', 125, 0.59082726368936045, 'corpus_27')]\n"
     ]
    }
   ],
   "source": [
    "# Sort the tfidif_tuples based on weight\n",
    "tfidf_tuples.sort(key=operator.itemgetter(0), reverse=True)\n",
    "tfidf_tuples.sort(key=operator.itemgetter(2), reverse=True)\n",
    "print('-----' * 8)\n",
    "print('Term and Weight for entire corpora')\n",
    "print('-----' * 8)\n",
    "\n",
    "# Get the top 5 words based on TF-IDF\n",
    "pprint(tfidf_tuples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
