{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sideka : Keyword Extraction for Each \"Desa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indonesia has thousands of villages (Bahasa = \"desa\"), the lowest level of government administration. While often portrayed as a beautiful and comfortable place for its citizens to live in, there are also some aspects of poverty and lag of infrastructure development that needs to be seriously considered. Since 2014, efforts have been made by Governments of Indonesia to improve the social welfare and quality of life as mandated by the Constitution. Unavailability of information is one major aspect contributing to this problem, and the Government, through the Village and Regional Empowerment Initiatives (\"Badan Prakarsa Pemberdayaan Desa dan Kawasan\" - BP2DK) launched \"Sistem Informasi Desa dan Kawasan\" (hereafter referred as \"SIDEKA\").  \n",
    "\n",
    "Until May 2018, SIDEKA has been utilized by around 4956 villages all around Indonesia. It provides a platform for the villages to monitor their activities, which afterward, the data can be compiled and used by the local government to form their Village Mid-Term Development Plans (\"Rencana Pembangunan Jangka Menengah Desa\" - RPJMDes), Village Government Activity Plans (\"Rencana Kegiatan Pemerintah Desa\" - RKPDes), and Village Revenue and Expenditure Budget Plan (\"Rencana Anggaran Pendapatan dan Belanja Desa\" - RAPBDes)\n",
    "\n",
    "Going forward, data from SIDEKA's implementations should be able to be utilized by higher-level administrative government such as District, Province, and Central Government so they can make a more tailored policy for each village according to their \"uniqueness\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems to be Tackled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniqueness. We are gathering all informations available from each village's website - specifically from their \"latest news\" section, compiling them, and extract keywords that are unique and can be a defining characteristic for the village.  \n",
    "  \n",
    "Example : Desa X defining keywords = \"Rengginang Ketan\" ; then we can point out to the policy makers higher ups to put more attention to this keyword since it may mean that aforementioned village economy leans heavily toward producing and selling this \"Rengginang Ketan\". Thus, subsequent policies created for this village should be accomodating to this fact (e.g. put more incentives for the villagers to create and sell more \"Rengginang Ketan\", more tax leeway, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use three methods often used to find the most important words across text documents:\n",
    "\n",
    "1. Word count\n",
    "2. Term Frequency - Inverse Document Frequency\n",
    "3. Rapid Automated Keyword Extraction\n",
    "\n",
    "Document sets used here are collection of articles scraped from Desa Pejeng (http://www.pejeng.desa.id/post/). Python is the language used to implement the aforementioned methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import glob\n",
    "import nltk\n",
    "import operator\n",
    "import csv\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Import word tokenizer packages\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from pprint import pprint\n",
    "\n",
    "# Import Dictionary, TfidfMode from gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Import RAKE package\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'scraped' article body: desa pejeng\n",
    "source_dir = 'pejeng_articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the news articles, sorted by last modification time: articles\n",
    "file_list = sorted(glob.glob(source_dir + '/*.txt'), key=os.path.getmtime)\n",
    "articles = [open(f, 'r').read() for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess articles: lowercasing and tokenizing all words\n",
    "articles_lower_tokenize = [word_tokenize(t.lower())\n",
    "                           for t in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess articles: removing 'indonesian' stopwords: articles_no_stop\n",
    "stopwords_indonesian = stopwords.words('indonesian')\n",
    "articles_no_stop = [[t for t in sublist if t not in stopwords_indonesian]\n",
    "                    for sublist in articles_lower_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess articles: removing punctuation\n",
    "articles_no_empty = [[t for t in sublist if t]\n",
    "                     for sublist in articles_no_stop]\n",
    "articles_no_empty_intermediate_1 = [[t for t in sublist if '``' not in t]\n",
    "                                    for sublist in articles_no_empty]\n",
    "articles_no_empty_intermediate_2 = [[t for t in sublist if '\\'\\'' not in t]\n",
    "                                    for sublist in articles_no_empty_intermediate_1]\n",
    "articles_cleaned = [[t for t in sublist if t not in punctuation]\n",
    "                    for sublist in articles_no_empty_intermediate_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Top 10 Words according to frequency:\n",
      "----------------------------------------\n",
      "[('pejeng', 88), ('dewa', 66), ('desa', 50), ('suamba', 43), ('banjar', 43), ('salah', 40), ('lapangan', 39), ('pura', 37), ('warga', 37), ('anak-anak', 36)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Looking up top 5 most-common words in the corpora\n",
    "# Create a counter object: counter\n",
    "counter = Counter([word for words in articles_cleaned for word in set(words)])\n",
    "print('-----' * 8)\n",
    "print(\"Top 10 Words according to frequency:\")\n",
    "print('-----' * 8)\n",
    "print(counter.most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Using Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim corpus and then apply Tfidf to that corpus\n",
    "# Create a (gensim) dictionary object from the articles_cleaned: dictionary\n",
    "dictionary = Dictionary(articles_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "TF-IDF Object from Corpus\n",
      "----------------------------------------\n",
      "TfidfModel(num_docs=89, num_nnz=11160) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tfidf object from corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "print('-----' * 8)\n",
    "print(\"TF-IDF Object from Corpus\")\n",
    "print('-----' * 8)\n",
    "print(tfidf, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TFIDF Weights of all terms found in corpus\n",
    "#  print as list of tuples, in descending order \n",
    "# Create a container for the list of tuples: tfidf_tuples\n",
    "tfidf_tuples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the cleaned articles\n",
    "# Get the top-5 of tfidf weight\n",
    "for i in range(len(articles_cleaned)):\n",
    "    doc = corpus[i]\n",
    "    tfidf_weights = tfidf[doc]\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "    #sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1])\n",
    "    #for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    for term_id, weight in sorted_tfidf_weights:\n",
    "        tfidf_tuples.append((dictionary.get(term_id), term_id, weight, 'corpus_{}'.format(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Term and Weight for entire corpora\n",
      "----------------------------------------\n",
      "[('elpiji', 2633, 0.85133269822700652, 'corpus_46'),\n",
      " ('kulkul', 834, 0.78491690292548111, 'corpus_48'),\n",
      " ('pengungsi', 2785, 0.72284623796788605, 'corpus_51'),\n",
      " ('ogoh-ogoh', 858, 0.66108899215597017, 'corpus_8'),\n",
      " ('topeng', 125, 0.59082726368936045, 'corpus_27')]\n"
     ]
    }
   ],
   "source": [
    "# Sort the tfidif_tuples based on weight\n",
    "tfidf_tuples.sort(key=operator.itemgetter(0), reverse=True)\n",
    "tfidf_tuples.sort(key=operator.itemgetter(2), reverse=True)\n",
    "print('-----' * 8)\n",
    "print('Term and Weight for entire corpora')\n",
    "print('-----' * 8)\n",
    "\n",
    "# Get the top 5 words based on TF-IDF\n",
    "pprint(tfidf_tuples[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAKE Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all articles into one\n",
    "articles_merged = ' '.join(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to remove punctuation between the sentences\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Import packages\n",
    "    import string, nltk, re\n",
    "    from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "    \n",
    "    # Create list of sentences from text\n",
    "    sent_tokenize_list = sent_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation from sentences\n",
    "    punct_pat = string.punctuation.replace('.', '')\n",
    "    pattern = r\"[{}]\".format(punct_pat) # create the pattern\n",
    "    \n",
    "    sent_tokenize_list_cleaned = []\n",
    "    for sent in sent_tokenize_list:\n",
    "        sent_lower = sent.lower()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(sent_lower)\n",
    "        filtered_words_sent = [w for w in tokens if not w in set(stopwords.words('indonesian'))]\n",
    "        filtered_sent = ' '.join(filtered_words_sent)\n",
    "        sent_cleaned = re.sub(pattern, ' ', filtered_sent) + '.'\n",
    "        sent_tokenize_list_cleaned.append(sent_cleaned)\n",
    "    \n",
    "    # Return cleaned sentence list\n",
    "    return ' '.join(sent_tokenize_list_cleaned)\n",
    "\n",
    "# Get the cleaned articles now\n",
    "articles_cleaned = preprocess(articles_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(87.24689240142831,\n",
      "  'krama desa pakraman jero kuta pejeng melaksanakan upacara bhuta yadnya '\n",
      "  'mecaru tawur kesanga bertepatan'),\n",
      " (84.2042108805315,\n",
      "  'persembahyangan sekda ida bagus giri putra menghaturkan dana punia diterima '\n",
      "  'langsung ngakan suardita'),\n",
      " (75.36680216802168,\n",
      "  'guru nabe ida pedanda manobawa griya bitera baleran menayakan terkait '\n",
      "  'motivasi'),\n",
      " (72.83458062709072,\n",
      "  'anak perguruan smp santi yoga pejeng gotong royong membersihkan areal tugu '\n",
      "  'pahlawan sapta dharma'),\n",
      " (70.14504452499547,\n",
      "  'diterima langsung bendesa pakraman jero kuta pejeng cokorda gede putra '\n",
      "  'pemayun')]\n"
     ]
    }
   ],
   "source": [
    "# Uses stopwords for english from NLTK, and all punctuation characters\n",
    "r = Rake(language='indonesian')\n",
    "\n",
    "# Extract keywords from non pre-processed articles\n",
    "r.extract_keywords_from_text(articles_merged)\n",
    "\n",
    "# Get keyword phrases ranked highest to lowest\n",
    "rake_score_precleaned = r.get_ranked_phrases_with_scores()[:5]\n",
    "pprint(rake_score_precleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1832.8510472913028,\n",
      "  'susunan pengurus stt yowana kertha yoga massa bhakti 2016 2021 ketua a a '\n",
      "  'gde juliana saputra wakil ketua i wayan agus widiana bendahara i ni kadek '\n",
      "  'nilayanti bendahara ii i gede dandy saputra sekretaris i i gusti ayu '\n",
      "  'manikasari sekretaris ii a a intan restika dewi serangkaian memeriahkan hut '\n",
      "  '47 yowana kertha yoga digelar lomba tarik tambang panjat pinang lainya'),\n",
      " (1095.2418147651183,\n",
      "  'sekretaris pdhb gianyar ida bagus kaimana sesuai surat keputusan pdhb '\n",
      "  'madiksa medwijati surat pernyataan guru nabe napak ida pedanda manobawa '\n",
      "  'girya bitra baleran upacara diksa diksita gelar bhiseka ida pedanda gde '\n",
      "  'buruan manuaba ida bagus cahyadi ida pedanda istri buruan ida ayu adnyani'),\n",
      " (1048.3142144646847,\n",
      "  'proyek rehabilitasi jalan banjar pande banjar puseh paket rehabilitasi '\n",
      "  'jalan pura dalem tarukan pejeng rehabilitasi jalan lingkungan desa sanding '\n",
      "  'rehabilitasi jalan lingkungan pengembungan pejeng kangin nilai anggaran rp '\n",
      "  '4 015 000 000 00 dewa suamba panas menyengat sembahyang teduh pejeng '\n",
      "  'suasana raya galungan kuningan desa pejeng beda perayaan'),\n",
      " (1007.2735761567162,\n",
      "  'i made suradnya m si gala siswa indonesia tingkat smp berharap membentuk '\n",
      "  'generasi muda berprestasi bidang olahraga sepakabola disamping perbekel '\n",
      "  'pejeng tjok agung kusuma yuda p hadir final gala siswa beliau berharap '\n",
      "  'kompetisi rutin diadakan tahunya mencari bibit bibit pemain sepakbola '\n",
      "  'berprestasi'),\n",
      " (982.1045875465499,\n",
      "  'padiksan sesuai sk parisada dharma hindu bali pdhb nomor 17 pdhb iv 2017 '\n",
      "  'dihadiri guru nabe ida pedanda manobawa griya bitera baleran guru waktra '\n",
      "  'ida pedanda jungutan griya peling padangtegal ubud guru saksi ida pedanda '\n",
      "  'wayahan dauh griya pitamaha siangan gianyar')]\n"
     ]
    }
   ],
   "source": [
    "# Uses stopwords for english from NLTK, and all punctuation characters\n",
    "r = Rake(language='indonesian')\n",
    "\n",
    "# Extract keywords from text from pre-processed articles\n",
    "r.extract_keywords_from_text(articles_cleaned)\n",
    "\n",
    "# Get keyword phrases ranked highest to lowest\n",
    "rake_score_cleaned = r.get_ranked_phrases_with_scores()[:5]\n",
    "pprint(rake_score_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
